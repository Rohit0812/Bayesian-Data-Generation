{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb11406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a759fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7d18911",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult.data.features\n",
    "y = adult.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f324c873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  education  education-num      marital-status  \\\n",
       "0       39         State-gov  Bachelors             13       Never-married   \n",
       "1       50  Self-emp-not-inc  Bachelors             13  Married-civ-spouse   \n",
       "2       38           Private    HS-grad              9            Divorced   \n",
       "3       53           Private       11th              7  Married-civ-spouse   \n",
       "4       28           Private  Bachelors             13  Married-civ-spouse   \n",
       "...    ...               ...        ...            ...                 ...   \n",
       "48837   39           Private  Bachelors             13            Divorced   \n",
       "48838   64               NaN    HS-grad              9             Widowed   \n",
       "48839   38           Private  Bachelors             13  Married-civ-spouse   \n",
       "48840   44           Private  Bachelors             13            Divorced   \n",
       "48841   35      Self-emp-inc  Bachelors             13  Married-civ-spouse   \n",
       "\n",
       "              occupation    relationship                race     sex  \\\n",
       "0           Adm-clerical   Not-in-family               White    Male   \n",
       "1        Exec-managerial         Husband               White    Male   \n",
       "2      Handlers-cleaners   Not-in-family               White    Male   \n",
       "3      Handlers-cleaners         Husband               Black    Male   \n",
       "4         Prof-specialty            Wife               Black  Female   \n",
       "...                  ...             ...                 ...     ...   \n",
       "48837     Prof-specialty   Not-in-family               White  Female   \n",
       "48838                NaN  Other-relative               Black    Male   \n",
       "48839     Prof-specialty         Husband               White    Male   \n",
       "48840       Adm-clerical       Own-child  Asian-Pac-Islander    Male   \n",
       "48841    Exec-managerial         Husband               White    Male   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country  \n",
       "0              2174             0              40  United-States  \n",
       "1                 0             0              13  United-States  \n",
       "2                 0             0              40  United-States  \n",
       "3                 0             0              40  United-States  \n",
       "4                 0             0              40           Cuba  \n",
       "...             ...           ...             ...            ...  \n",
       "48837             0             0              36  United-States  \n",
       "48838             0             0              40  United-States  \n",
       "48839             0             0              50  United-States  \n",
       "48840          5455             0              40  United-States  \n",
       "48841             0             0              60  United-States  \n",
       "\n",
       "[48842 rows x 13 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c29fd5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age size: 74\n",
      "workclass size: 10\n",
      "fnlwgt size: 28523\n",
      "education size: 16\n",
      "education-num size: 16\n",
      "marital-status size: 7\n",
      "occupation size: 16\n",
      "relationship size: 6\n",
      "race size: 5\n",
      "sex size: 2\n",
      "capital-gain size: 123\n",
      "capital-loss size: 99\n",
      "hours-per-week size: 96\n",
      "native-country size: 43\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: X[col].unique() for col in X.columns}\n",
    "\n",
    "# Optional: print in readable format\n",
    "for col, values in distinct_values.items():\n",
    "    print(f\"{col} size: {len(values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3f80c",
   "metadata": {},
   "source": [
    "All except fnlwgt are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbe94533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishali Rathore\\AppData\\Local\\Temp\\ipykernel_4748\\650624975.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.drop('fnlwgt', inplace=True, axis=1)\n"
     ]
    }
   ],
   "source": [
    "X.drop('fnlwgt', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55a3b3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'education', 'education-num', 'marital-status',\n",
       "       'occupation', 'relationship', 'race', 'sex', 'capital-gain',\n",
       "       'capital-loss', 'hours-per-week', 'native-country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_columns = X.columns\n",
    "discrete_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "52a82d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ctgan.data_transformer import DataTransformer\n",
    "\n",
    "transformer = DataTransformer()\n",
    "transformer.fit(X, discrete_columns)\n",
    "train_data = transformer.transform(X)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7c0e5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SpanInfo(dim=74, activation_fn='softmax')],\n",
       " [SpanInfo(dim=10, activation_fn='softmax')],\n",
       " [SpanInfo(dim=16, activation_fn='softmax')],\n",
       " [SpanInfo(dim=16, activation_fn='softmax')],\n",
       " [SpanInfo(dim=7, activation_fn='softmax')],\n",
       " [SpanInfo(dim=16, activation_fn='softmax')],\n",
       " [SpanInfo(dim=6, activation_fn='softmax')],\n",
       " [SpanInfo(dim=5, activation_fn='softmax')],\n",
       " [SpanInfo(dim=2, activation_fn='softmax')],\n",
       " [SpanInfo(dim=123, activation_fn='softmax')],\n",
       " [SpanInfo(dim=99, activation_fn='softmax')],\n",
       " [SpanInfo(dim=96, activation_fn='softmax')],\n",
       " [SpanInfo(dim=43, activation_fn='softmax')]]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.output_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "09846740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan.data_sampler import DataSampler\n",
    "\n",
    "data_sampler = DataSampler(train_data, transformer.output_info_list, log_frequency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b2b3aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 513) (3, 13) [7 5 7] [1 9 0]\n"
     ]
    }
   ],
   "source": [
    "c1, m1, col, opt = data_sampler.sample_condvec(3)\n",
    "print(c1.shape, m1.shape, col, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8c394619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], D Loss: -1.1629, G Loss: 1.2222, KL Div: 117.2592\n",
      "Epoch [2/300], D Loss: 0.0464, G Loss: 1.5376, KL Div: 89.6965\n",
      "Epoch [3/300], D Loss: 0.0601, G Loss: 1.2585, KL Div: 76.8216\n",
      "Epoch [4/300], D Loss: 0.0739, G Loss: 1.6931, KL Div: 67.9400\n",
      "Epoch [5/300], D Loss: 0.1331, G Loss: 1.6046, KL Div: 62.2727\n",
      "Epoch [6/300], D Loss: 0.0905, G Loss: 1.4813, KL Div: 59.1348\n",
      "Epoch [7/300], D Loss: -0.0828, G Loss: 1.7664, KL Div: 58.4815\n",
      "Epoch [8/300], D Loss: -0.0865, G Loss: 1.9000, KL Div: 60.3811\n",
      "Epoch [9/300], D Loss: 0.0575, G Loss: 1.6614, KL Div: 65.1289\n",
      "Epoch [10/300], D Loss: -0.1302, G Loss: 1.6682, KL Div: 72.5939\n",
      "Epoch [11/300], D Loss: -0.0907, G Loss: 1.4978, KL Div: 80.8934\n",
      "Epoch [12/300], D Loss: -0.0232, G Loss: 1.2334, KL Div: 86.8840\n",
      "Epoch [13/300], D Loss: 0.1047, G Loss: 1.1794, KL Div: 90.2241\n",
      "Epoch [14/300], D Loss: -0.0516, G Loss: 0.5057, KL Div: 91.6800\n",
      "Epoch [15/300], D Loss: 0.0612, G Loss: 1.2616, KL Div: 92.0751\n",
      "Epoch [16/300], D Loss: -0.2855, G Loss: 0.5567, KL Div: 92.3033\n",
      "Epoch [17/300], D Loss: -0.2249, G Loss: 1.1176, KL Div: 93.0763\n",
      "Epoch [18/300], D Loss: 0.0518, G Loss: 0.1311, KL Div: 94.7394\n",
      "Epoch [19/300], D Loss: -0.0980, G Loss: 0.8096, KL Div: 97.1531\n",
      "Epoch [20/300], D Loss: -0.1360, G Loss: 0.6569, KL Div: 100.2409\n",
      "Epoch [21/300], D Loss: -0.1743, G Loss: 0.6515, KL Div: 103.1143\n",
      "Epoch [22/300], D Loss: -0.1861, G Loss: 0.2885, KL Div: 105.2330\n",
      "Epoch [23/300], D Loss: -0.1283, G Loss: 0.6831, KL Div: 107.1439\n",
      "Epoch [24/300], D Loss: -0.0720, G Loss: 0.5670, KL Div: 109.1155\n",
      "Epoch [25/300], D Loss: 0.0973, G Loss: 0.4757, KL Div: 110.4865\n",
      "Epoch [26/300], D Loss: -0.2232, G Loss: 1.0947, KL Div: 111.6765\n",
      "Epoch [27/300], D Loss: -0.0344, G Loss: 0.5952, KL Div: 113.2806\n",
      "Epoch [28/300], D Loss: -0.3215, G Loss: 0.5939, KL Div: 114.0844\n",
      "Epoch [29/300], D Loss: -0.2889, G Loss: 0.9383, KL Div: 115.1594\n",
      "Epoch [30/300], D Loss: -0.0689, G Loss: 0.1843, KL Div: 115.8874\n",
      "Epoch [31/300], D Loss: -0.3547, G Loss: 0.5269, KL Div: 116.3663\n",
      "Epoch [32/300], D Loss: -0.2306, G Loss: 0.5571, KL Div: 116.3372\n",
      "Epoch [33/300], D Loss: -0.4255, G Loss: 0.4736, KL Div: 116.9357\n",
      "Epoch [34/300], D Loss: -0.1404, G Loss: 0.6188, KL Div: 117.1245\n",
      "Epoch [35/300], D Loss: -0.2733, G Loss: 0.3740, KL Div: 117.4124\n",
      "Epoch [36/300], D Loss: 0.3014, G Loss: 0.5606, KL Div: 117.1356\n",
      "Epoch [37/300], D Loss: -0.0251, G Loss: 0.4775, KL Div: 117.0214\n",
      "Epoch [38/300], D Loss: -0.2371, G Loss: 0.3865, KL Div: 116.8279\n",
      "Epoch [39/300], D Loss: 0.1913, G Loss: 0.5372, KL Div: 116.9140\n",
      "Epoch [40/300], D Loss: 0.0042, G Loss: 0.1906, KL Div: 116.8578\n",
      "Epoch [41/300], D Loss: -0.2082, G Loss: 0.2643, KL Div: 116.3528\n",
      "Epoch [42/300], D Loss: 0.1198, G Loss: 0.3210, KL Div: 116.0644\n",
      "Epoch [43/300], D Loss: -0.0505, G Loss: 0.3056, KL Div: 116.0987\n",
      "Epoch [44/300], D Loss: -0.2205, G Loss: 0.4512, KL Div: 115.9660\n",
      "Epoch [45/300], D Loss: -0.0829, G Loss: 0.3833, KL Div: 115.6920\n",
      "Epoch [46/300], D Loss: -0.6106, G Loss: 0.3618, KL Div: 115.2734\n",
      "Epoch [47/300], D Loss: 0.0648, G Loss: 0.3446, KL Div: 114.8097\n",
      "Epoch [48/300], D Loss: -0.3432, G Loss: 0.0251, KL Div: 114.2855\n",
      "Epoch [49/300], D Loss: -0.0534, G Loss: -0.0169, KL Div: 114.0494\n",
      "Epoch [50/300], D Loss: -0.2746, G Loss: 0.1213, KL Div: 113.6735\n",
      "Epoch [51/300], D Loss: -0.3197, G Loss: 0.4706, KL Div: 113.2072\n",
      "Epoch [52/300], D Loss: 0.0733, G Loss: 0.1641, KL Div: 112.9496\n",
      "Epoch [53/300], D Loss: -0.3881, G Loss: -0.2334, KL Div: 112.5605\n",
      "Epoch [54/300], D Loss: -0.2772, G Loss: 0.2765, KL Div: 112.1154\n",
      "Epoch [55/300], D Loss: -0.0085, G Loss: 0.3061, KL Div: 111.8295\n",
      "Epoch [56/300], D Loss: -0.3483, G Loss: -0.2101, KL Div: 111.5351\n",
      "Epoch [57/300], D Loss: -0.2880, G Loss: 0.2171, KL Div: 111.2592\n",
      "Epoch [58/300], D Loss: -0.6195, G Loss: 0.1512, KL Div: 111.0834\n",
      "Epoch [59/300], D Loss: -0.2379, G Loss: 0.2108, KL Div: 110.4242\n",
      "Epoch [60/300], D Loss: -0.3622, G Loss: 0.0538, KL Div: 110.3704\n",
      "Epoch [61/300], D Loss: -0.0595, G Loss: 0.2293, KL Div: 110.1106\n",
      "Epoch [62/300], D Loss: -0.0927, G Loss: 0.6381, KL Div: 110.1312\n",
      "Epoch [63/300], D Loss: -0.3068, G Loss: 0.3861, KL Div: 110.1226\n",
      "Epoch [64/300], D Loss: -0.4247, G Loss: 0.0555, KL Div: 109.7520\n",
      "Epoch [65/300], D Loss: -0.0898, G Loss: 0.1404, KL Div: 109.5696\n",
      "Epoch [66/300], D Loss: -0.0834, G Loss: 0.0215, KL Div: 109.2769\n",
      "Epoch [67/300], D Loss: -0.3690, G Loss: -0.2284, KL Div: 109.0281\n",
      "Epoch [68/300], D Loss: -0.2494, G Loss: 0.0942, KL Div: 108.7941\n",
      "Epoch [69/300], D Loss: -0.2126, G Loss: -0.0334, KL Div: 108.6522\n",
      "Epoch [70/300], D Loss: 0.0507, G Loss: -0.0056, KL Div: 108.2594\n",
      "Epoch [71/300], D Loss: -0.1760, G Loss: 0.1719, KL Div: 108.1055\n",
      "Epoch [72/300], D Loss: -0.0899, G Loss: 0.2211, KL Div: 108.0053\n",
      "Epoch [73/300], D Loss: -0.2377, G Loss: 0.0997, KL Div: 107.8308\n",
      "Epoch [74/300], D Loss: -0.2566, G Loss: -0.3546, KL Div: 107.3093\n",
      "Epoch [75/300], D Loss: -0.1969, G Loss: 0.0733, KL Div: 107.2764\n",
      "Epoch [76/300], D Loss: 0.0333, G Loss: 0.0336, KL Div: 107.1456\n",
      "Epoch [77/300], D Loss: -0.1551, G Loss: 0.0153, KL Div: 106.9478\n",
      "Epoch [78/300], D Loss: 0.0023, G Loss: -0.1282, KL Div: 106.9649\n",
      "Epoch [79/300], D Loss: -0.3063, G Loss: -0.3664, KL Div: 106.7401\n",
      "Epoch [80/300], D Loss: -0.0616, G Loss: -0.0759, KL Div: 106.8184\n",
      "Epoch [81/300], D Loss: -0.2460, G Loss: -0.3447, KL Div: 106.5704\n",
      "Epoch [82/300], D Loss: -0.3059, G Loss: -0.2051, KL Div: 106.2018\n",
      "Epoch [83/300], D Loss: 0.1087, G Loss: -0.4401, KL Div: 106.2892\n",
      "Epoch [84/300], D Loss: -0.0047, G Loss: -0.4046, KL Div: 106.3826\n",
      "Epoch [85/300], D Loss: -0.2141, G Loss: -0.4768, KL Div: 105.9951\n",
      "Epoch [86/300], D Loss: -0.3671, G Loss: -0.3682, KL Div: 106.2194\n",
      "Epoch [87/300], D Loss: -0.2399, G Loss: -0.6221, KL Div: 105.9235\n",
      "Epoch [88/300], D Loss: -0.1777, G Loss: -0.2442, KL Div: 105.6635\n",
      "Epoch [89/300], D Loss: 0.1140, G Loss: -0.4142, KL Div: 105.7959\n",
      "Epoch [90/300], D Loss: 0.1705, G Loss: -0.4429, KL Div: 105.4227\n",
      "Epoch [91/300], D Loss: -0.2684, G Loss: -0.0967, KL Div: 105.2563\n",
      "Epoch [92/300], D Loss: -0.1585, G Loss: -0.0109, KL Div: 105.0530\n",
      "Epoch [93/300], D Loss: -0.1894, G Loss: -0.3678, KL Div: 105.1606\n",
      "Epoch [94/300], D Loss: -0.3496, G Loss: -0.4467, KL Div: 104.9691\n",
      "Epoch [95/300], D Loss: -0.2898, G Loss: -0.4560, KL Div: 104.9107\n",
      "Epoch [96/300], D Loss: 0.0428, G Loss: -0.8423, KL Div: 104.5546\n",
      "Epoch [97/300], D Loss: -0.0966, G Loss: -0.4857, KL Div: 104.3702\n",
      "Epoch [98/300], D Loss: -0.1071, G Loss: -0.3238, KL Div: 104.0942\n",
      "Epoch [99/300], D Loss: -0.1870, G Loss: -0.1779, KL Div: 104.1089\n",
      "Epoch [100/300], D Loss: -0.0425, G Loss: -0.2095, KL Div: 103.6737\n",
      "Epoch [101/300], D Loss: -0.2176, G Loss: -0.2710, KL Div: 103.6016\n",
      "Epoch [102/300], D Loss: -0.2345, G Loss: -0.2578, KL Div: 103.4193\n",
      "Epoch [103/300], D Loss: -0.4120, G Loss: -0.4056, KL Div: 103.3133\n",
      "Epoch [104/300], D Loss: -0.1681, G Loss: -0.2961, KL Div: 103.1194\n",
      "Epoch [105/300], D Loss: -0.0892, G Loss: -0.3068, KL Div: 102.8213\n",
      "Epoch [106/300], D Loss: -0.0501, G Loss: -0.0043, KL Div: 102.6921\n",
      "Epoch [107/300], D Loss: -0.3020, G Loss: -0.3950, KL Div: 102.6302\n",
      "Epoch [108/300], D Loss: -0.2697, G Loss: -0.4349, KL Div: 102.5533\n",
      "Epoch [109/300], D Loss: -0.3157, G Loss: -0.4282, KL Div: 102.4104\n",
      "Epoch [110/300], D Loss: -0.5350, G Loss: -0.5034, KL Div: 101.8897\n",
      "Epoch [111/300], D Loss: -0.5167, G Loss: -0.5852, KL Div: 101.8870\n",
      "Epoch [112/300], D Loss: -0.2316, G Loss: -0.6483, KL Div: 101.9474\n",
      "Epoch [113/300], D Loss: -0.0865, G Loss: -0.2140, KL Div: 101.3035\n",
      "Epoch [114/300], D Loss: -0.1775, G Loss: -0.4859, KL Div: 101.5183\n",
      "Epoch [115/300], D Loss: -0.2334, G Loss: -0.3331, KL Div: 101.5109\n",
      "Epoch [116/300], D Loss: -0.3532, G Loss: 0.0100, KL Div: 101.3974\n",
      "Epoch [117/300], D Loss: -0.0760, G Loss: -0.2354, KL Div: 101.3033\n",
      "Epoch [118/300], D Loss: -0.3270, G Loss: -0.3544, KL Div: 101.1354\n",
      "Epoch [119/300], D Loss: -0.1210, G Loss: -0.7430, KL Div: 101.1130\n",
      "Epoch [120/300], D Loss: -0.3124, G Loss: -0.3635, KL Div: 100.9619\n",
      "Epoch [121/300], D Loss: -0.1196, G Loss: -0.1531, KL Div: 100.8331\n",
      "Epoch [122/300], D Loss: -0.1322, G Loss: -0.2512, KL Div: 100.3292\n",
      "Epoch [123/300], D Loss: -0.0111, G Loss: -0.1779, KL Div: 100.2135\n",
      "Epoch [124/300], D Loss: -0.0985, G Loss: -0.2794, KL Div: 100.4327\n",
      "Epoch [125/300], D Loss: -0.2606, G Loss: -0.1491, KL Div: 100.0516\n",
      "Epoch [126/300], D Loss: -0.2968, G Loss: -0.3296, KL Div: 100.2826\n",
      "Epoch [127/300], D Loss: -0.1897, G Loss: -0.6548, KL Div: 100.0514\n",
      "Epoch [128/300], D Loss: -0.0919, G Loss: -0.4890, KL Div: 99.7966\n",
      "Epoch [129/300], D Loss: -0.3117, G Loss: -0.6717, KL Div: 99.5639\n",
      "Epoch [130/300], D Loss: -0.2567, G Loss: -0.3221, KL Div: 99.3917\n",
      "Epoch [131/300], D Loss: -0.1876, G Loss: -0.2020, KL Div: 99.3114\n",
      "Epoch [132/300], D Loss: -0.1053, G Loss: -0.2598, KL Div: 99.3238\n",
      "Epoch [133/300], D Loss: -0.2827, G Loss: -0.2294, KL Div: 98.8938\n",
      "Epoch [134/300], D Loss: -0.0983, G Loss: -0.3027, KL Div: 98.9385\n",
      "Epoch [135/300], D Loss: -0.1222, G Loss: -0.4913, KL Div: 98.4685\n",
      "Epoch [136/300], D Loss: -0.3337, G Loss: -0.9473, KL Div: 98.5513\n",
      "Epoch [137/300], D Loss: -0.3039, G Loss: -0.6981, KL Div: 98.4102\n",
      "Epoch [138/300], D Loss: -0.3067, G Loss: -0.5601, KL Div: 98.1808\n",
      "Epoch [139/300], D Loss: -0.1145, G Loss: -0.9793, KL Div: 98.0509\n",
      "Epoch [140/300], D Loss: -0.2383, G Loss: -0.8136, KL Div: 97.8323\n",
      "Epoch [141/300], D Loss: -0.3005, G Loss: -0.6728, KL Div: 97.7238\n",
      "Epoch [142/300], D Loss: -0.3346, G Loss: -0.6397, KL Div: 97.4484\n",
      "Epoch [143/300], D Loss: -0.2299, G Loss: -0.7199, KL Div: 97.1891\n",
      "Epoch [144/300], D Loss: -0.4367, G Loss: -0.3823, KL Div: 97.0088\n",
      "Epoch [145/300], D Loss: -0.1259, G Loss: -0.4518, KL Div: 96.8863\n",
      "Epoch [146/300], D Loss: -0.1941, G Loss: -0.5913, KL Div: 96.6156\n",
      "Epoch [147/300], D Loss: -0.3401, G Loss: -0.5588, KL Div: 96.7499\n",
      "Epoch [148/300], D Loss: -0.3307, G Loss: -0.4696, KL Div: 96.4537\n",
      "Epoch [149/300], D Loss: -0.2393, G Loss: -0.9332, KL Div: 96.2665\n",
      "Epoch [150/300], D Loss: -0.1638, G Loss: -0.6675, KL Div: 96.2207\n",
      "Epoch [151/300], D Loss: -0.2588, G Loss: -0.2734, KL Div: 96.0341\n",
      "Epoch [152/300], D Loss: 0.0058, G Loss: -0.4268, KL Div: 96.0614\n",
      "Epoch [153/300], D Loss: -0.1085, G Loss: -0.3292, KL Div: 95.8668\n",
      "Epoch [154/300], D Loss: -0.0283, G Loss: -0.3015, KL Div: 95.8861\n",
      "Epoch [155/300], D Loss: -0.3594, G Loss: -0.4520, KL Div: 95.6731\n",
      "Epoch [156/300], D Loss: -0.0447, G Loss: -0.4225, KL Div: 95.7479\n",
      "Epoch [157/300], D Loss: -0.1386, G Loss: -0.7120, KL Div: 95.5724\n",
      "Epoch [158/300], D Loss: -0.2101, G Loss: -0.5426, KL Div: 95.3503\n",
      "Epoch [159/300], D Loss: -0.2697, G Loss: -0.7330, KL Div: 94.9800\n",
      "Epoch [160/300], D Loss: -0.0459, G Loss: -0.4211, KL Div: 94.5561\n",
      "Epoch [161/300], D Loss: -0.3404, G Loss: -0.2103, KL Div: 94.4036\n",
      "Epoch [162/300], D Loss: -0.2137, G Loss: -0.2205, KL Div: 94.2286\n",
      "Epoch [163/300], D Loss: -0.1621, G Loss: -0.5496, KL Div: 94.0966\n",
      "Epoch [164/300], D Loss: 0.0633, G Loss: -0.6941, KL Div: 93.9301\n",
      "Epoch [165/300], D Loss: -0.2205, G Loss: -0.5233, KL Div: 93.6469\n",
      "Epoch [166/300], D Loss: -0.1864, G Loss: -0.7186, KL Div: 93.5154\n",
      "Epoch [167/300], D Loss: 0.1524, G Loss: -0.8755, KL Div: 93.4645\n",
      "Epoch [168/300], D Loss: -0.0251, G Loss: -0.6268, KL Div: 93.1789\n",
      "Epoch [169/300], D Loss: -0.3403, G Loss: -0.4716, KL Div: 93.0385\n",
      "Epoch [170/300], D Loss: 0.0031, G Loss: -0.4777, KL Div: 92.8984\n",
      "Epoch [171/300], D Loss: -0.0274, G Loss: -0.3753, KL Div: 92.6245\n",
      "Epoch [172/300], D Loss: -0.1377, G Loss: -0.5581, KL Div: 92.6427\n",
      "Epoch [173/300], D Loss: -0.4436, G Loss: -0.5424, KL Div: 92.4646\n",
      "Epoch [174/300], D Loss: -0.0917, G Loss: -0.7403, KL Div: 92.3637\n",
      "Epoch [175/300], D Loss: -0.2019, G Loss: -0.9295, KL Div: 92.2013\n",
      "Epoch [176/300], D Loss: -0.1699, G Loss: -0.9920, KL Div: 92.1889\n",
      "Epoch [177/300], D Loss: -0.2503, G Loss: -0.6437, KL Div: 91.9564\n",
      "Epoch [178/300], D Loss: -0.1100, G Loss: -0.6591, KL Div: 91.7946\n",
      "Epoch [179/300], D Loss: -0.2407, G Loss: -0.4482, KL Div: 91.4666\n",
      "Epoch [180/300], D Loss: -0.2727, G Loss: -0.6472, KL Div: 91.1591\n",
      "Epoch [181/300], D Loss: -0.2011, G Loss: -0.5120, KL Div: 90.9675\n",
      "Epoch [182/300], D Loss: -0.1200, G Loss: -0.4334, KL Div: 90.6807\n",
      "Epoch [183/300], D Loss: -0.2019, G Loss: -1.0025, KL Div: 90.7170\n",
      "Epoch [184/300], D Loss: -0.1532, G Loss: -0.8746, KL Div: 90.5464\n",
      "Epoch [185/300], D Loss: -0.0589, G Loss: -0.7255, KL Div: 90.4140\n",
      "Epoch [186/300], D Loss: -0.2276, G Loss: -0.4411, KL Div: 90.2347\n",
      "Epoch [187/300], D Loss: -0.2232, G Loss: -0.8196, KL Div: 89.9495\n",
      "Epoch [188/300], D Loss: 0.0534, G Loss: -0.4679, KL Div: 89.7282\n",
      "Epoch [189/300], D Loss: -0.0819, G Loss: -0.8492, KL Div: 89.6848\n",
      "Epoch [190/300], D Loss: -0.1535, G Loss: -0.5806, KL Div: 89.3202\n",
      "Epoch [191/300], D Loss: -0.1875, G Loss: -0.4688, KL Div: 89.0759\n",
      "Epoch [192/300], D Loss: -0.0047, G Loss: -0.7180, KL Div: 88.9172\n",
      "Epoch [193/300], D Loss: -0.1275, G Loss: -0.5261, KL Div: 88.6805\n",
      "Epoch [194/300], D Loss: -0.2508, G Loss: -0.3536, KL Div: 88.5417\n",
      "Epoch [195/300], D Loss: 0.0046, G Loss: -0.5053, KL Div: 88.2437\n",
      "Epoch [196/300], D Loss: -0.0360, G Loss: -0.4842, KL Div: 88.2115\n",
      "Epoch [197/300], D Loss: -0.2141, G Loss: -0.9171, KL Div: 88.1398\n",
      "Epoch [198/300], D Loss: 0.0575, G Loss: -0.7555, KL Div: 87.9096\n",
      "Epoch [199/300], D Loss: -0.1582, G Loss: -0.5971, KL Div: 87.7610\n",
      "Epoch [200/300], D Loss: -0.0218, G Loss: -0.5284, KL Div: 87.4084\n",
      "Epoch [201/300], D Loss: -0.0049, G Loss: -0.6242, KL Div: 87.2979\n",
      "Epoch [202/300], D Loss: -0.2043, G Loss: -0.5088, KL Div: 87.1531\n",
      "Epoch [203/300], D Loss: -0.0233, G Loss: -0.5074, KL Div: 87.1427\n",
      "Epoch [204/300], D Loss: -0.2556, G Loss: -0.6322, KL Div: 87.0419\n",
      "Epoch [205/300], D Loss: -0.4043, G Loss: -0.7331, KL Div: 86.8472\n",
      "Epoch [206/300], D Loss: -0.1223, G Loss: -0.6806, KL Div: 86.4940\n",
      "Epoch [207/300], D Loss: -0.1817, G Loss: -0.4730, KL Div: 86.2777\n",
      "Epoch [208/300], D Loss: -0.1969, G Loss: -0.5570, KL Div: 86.0602\n",
      "Epoch [209/300], D Loss: -0.2079, G Loss: -0.5778, KL Div: 85.8390\n",
      "Epoch [210/300], D Loss: -0.1231, G Loss: -0.3531, KL Div: 85.7277\n",
      "Epoch [211/300], D Loss: -0.0240, G Loss: -0.2533, KL Div: 85.6762\n",
      "Epoch [212/300], D Loss: -0.2624, G Loss: -0.3956, KL Div: 85.5452\n",
      "Epoch [213/300], D Loss: -0.1709, G Loss: -0.6690, KL Div: 85.2373\n",
      "Epoch [214/300], D Loss: 0.0238, G Loss: -0.6155, KL Div: 85.2438\n",
      "Epoch [215/300], D Loss: -0.0337, G Loss: -0.7623, KL Div: 85.2714\n",
      "Epoch [216/300], D Loss: 0.0898, G Loss: -0.8159, KL Div: 84.9980\n",
      "Epoch [217/300], D Loss: -0.1880, G Loss: -0.7186, KL Div: 84.8378\n",
      "Epoch [218/300], D Loss: -0.2956, G Loss: -1.0800, KL Div: 84.4480\n",
      "Epoch [219/300], D Loss: -0.2511, G Loss: -1.2042, KL Div: 84.3036\n",
      "Epoch [220/300], D Loss: -0.2264, G Loss: -0.8347, KL Div: 84.0891\n",
      "Epoch [221/300], D Loss: -0.0678, G Loss: -0.8283, KL Div: 83.9417\n",
      "Epoch [222/300], D Loss: -0.1141, G Loss: -0.7724, KL Div: 83.9328\n",
      "Epoch [223/300], D Loss: -0.1389, G Loss: -0.8310, KL Div: 83.7938\n",
      "Epoch [224/300], D Loss: -0.0989, G Loss: -0.7281, KL Div: 83.6818\n",
      "Epoch [225/300], D Loss: -0.0815, G Loss: -0.6184, KL Div: 83.3680\n",
      "Epoch [226/300], D Loss: 0.0243, G Loss: -0.7690, KL Div: 83.0749\n",
      "Epoch [227/300], D Loss: -0.1060, G Loss: -0.5690, KL Div: 83.1256\n",
      "Epoch [228/300], D Loss: -0.0540, G Loss: -0.3931, KL Div: 82.9010\n",
      "Epoch [229/300], D Loss: 0.0421, G Loss: -0.6723, KL Div: 82.7629\n",
      "Epoch [230/300], D Loss: -0.0756, G Loss: -0.9427, KL Div: 82.7391\n",
      "Epoch [231/300], D Loss: -0.2953, G Loss: -0.6656, KL Div: 82.5007\n",
      "Epoch [232/300], D Loss: -0.0927, G Loss: -0.7350, KL Div: 82.4173\n",
      "Epoch [233/300], D Loss: -0.1991, G Loss: -0.8396, KL Div: 82.2294\n",
      "Epoch [234/300], D Loss: 0.0504, G Loss: -0.5472, KL Div: 82.1180\n",
      "Epoch [235/300], D Loss: -0.1811, G Loss: -0.7102, KL Div: 82.0751\n",
      "Epoch [236/300], D Loss: -0.0214, G Loss: -0.7062, KL Div: 81.7764\n",
      "Epoch [237/300], D Loss: -0.1485, G Loss: -0.9031, KL Div: 81.7501\n",
      "Epoch [238/300], D Loss: -0.1863, G Loss: -0.8184, KL Div: 81.6425\n",
      "Epoch [239/300], D Loss: -0.1090, G Loss: -0.8873, KL Div: 81.4440\n",
      "Epoch [240/300], D Loss: -0.2532, G Loss: -0.5962, KL Div: 81.2261\n",
      "Epoch [241/300], D Loss: 0.0024, G Loss: -0.4388, KL Div: 81.2662\n",
      "Epoch [242/300], D Loss: 0.0156, G Loss: -0.5893, KL Div: 81.0455\n",
      "Epoch [243/300], D Loss: -0.1806, G Loss: -0.5841, KL Div: 80.8593\n",
      "Epoch [244/300], D Loss: -0.3992, G Loss: -0.4340, KL Div: 80.6774\n",
      "Epoch [245/300], D Loss: -0.2005, G Loss: -0.5633, KL Div: 80.5487\n",
      "Epoch [246/300], D Loss: -0.1860, G Loss: -0.7961, KL Div: 80.4522\n",
      "Epoch [247/300], D Loss: -0.3218, G Loss: -0.5975, KL Div: 80.2578\n",
      "Epoch [248/300], D Loss: -0.1568, G Loss: -0.8300, KL Div: 80.1167\n",
      "Epoch [249/300], D Loss: -0.1287, G Loss: -0.7787, KL Div: 80.0917\n",
      "Epoch [250/300], D Loss: -0.3601, G Loss: -0.6029, KL Div: 80.0543\n",
      "Epoch [251/300], D Loss: -0.0819, G Loss: -0.8682, KL Div: 79.8649\n",
      "Epoch [252/300], D Loss: -0.2947, G Loss: -0.5603, KL Div: 79.8889\n",
      "Epoch [253/300], D Loss: -0.1039, G Loss: -0.4899, KL Div: 79.6364\n",
      "Epoch [254/300], D Loss: -0.0125, G Loss: -1.0438, KL Div: 79.5114\n",
      "Epoch [255/300], D Loss: 0.0123, G Loss: -0.6340, KL Div: 79.3496\n",
      "Epoch [256/300], D Loss: -0.3134, G Loss: -0.7535, KL Div: 79.2825\n",
      "Epoch [257/300], D Loss: -0.2273, G Loss: -0.7513, KL Div: 79.1805\n",
      "Epoch [258/300], D Loss: -0.3257, G Loss: -0.9787, KL Div: 78.9624\n",
      "Epoch [259/300], D Loss: -0.0739, G Loss: -0.5951, KL Div: 78.8080\n",
      "Epoch [260/300], D Loss: -0.3590, G Loss: -0.6142, KL Div: 78.3980\n",
      "Epoch [261/300], D Loss: -0.1657, G Loss: -0.3690, KL Div: 78.4908\n",
      "Epoch [262/300], D Loss: -0.0404, G Loss: -0.5841, KL Div: 78.4310\n",
      "Epoch [263/300], D Loss: -0.2672, G Loss: -0.6746, KL Div: 78.3769\n",
      "Epoch [264/300], D Loss: -0.0134, G Loss: -0.6969, KL Div: 78.2768\n",
      "Epoch [265/300], D Loss: -0.0130, G Loss: -0.6538, KL Div: 78.1542\n",
      "Epoch [266/300], D Loss: -0.1164, G Loss: -0.9652, KL Div: 78.0753\n",
      "Epoch [267/300], D Loss: -0.2731, G Loss: -1.0348, KL Div: 77.9359\n",
      "Epoch [268/300], D Loss: 0.1777, G Loss: -0.9216, KL Div: 77.7820\n",
      "Epoch [269/300], D Loss: -0.2199, G Loss: -0.4577, KL Div: 77.4800\n",
      "Epoch [270/300], D Loss: -0.1317, G Loss: -0.7099, KL Div: 77.5747\n",
      "Epoch [271/300], D Loss: -0.1290, G Loss: -0.7392, KL Div: 77.4438\n",
      "Epoch [272/300], D Loss: -0.2221, G Loss: -1.1092, KL Div: 77.2305\n",
      "Epoch [273/300], D Loss: -0.3105, G Loss: -0.8796, KL Div: 77.1129\n",
      "Epoch [274/300], D Loss: -0.1691, G Loss: -0.8797, KL Div: 76.8701\n",
      "Epoch [275/300], D Loss: -0.0005, G Loss: -0.7152, KL Div: 76.6859\n",
      "Epoch [276/300], D Loss: -0.1753, G Loss: -0.7743, KL Div: 76.6897\n",
      "Epoch [277/300], D Loss: -0.0557, G Loss: -0.3707, KL Div: 76.6855\n",
      "Epoch [278/300], D Loss: -0.2431, G Loss: -0.9373, KL Div: 76.4188\n",
      "Epoch [279/300], D Loss: -0.3519, G Loss: -1.1578, KL Div: 76.4036\n",
      "Epoch [280/300], D Loss: -0.2470, G Loss: -0.6431, KL Div: 76.3194\n",
      "Epoch [281/300], D Loss: -0.1922, G Loss: -0.6523, KL Div: 76.0865\n",
      "Epoch [282/300], D Loss: -0.1086, G Loss: -0.7298, KL Div: 75.9480\n",
      "Epoch [283/300], D Loss: -0.3379, G Loss: -0.6633, KL Div: 75.8467\n",
      "Epoch [284/300], D Loss: -0.1428, G Loss: -0.8174, KL Div: 75.6861\n",
      "Epoch [285/300], D Loss: -0.2010, G Loss: -0.5136, KL Div: 75.4972\n",
      "Epoch [286/300], D Loss: -0.0449, G Loss: -0.7208, KL Div: 75.2588\n",
      "Epoch [287/300], D Loss: -0.2651, G Loss: -0.6659, KL Div: 75.1025\n",
      "Epoch [288/300], D Loss: -0.0487, G Loss: -0.7678, KL Div: 74.9980\n",
      "Epoch [289/300], D Loss: -0.1866, G Loss: -1.1173, KL Div: 74.8942\n",
      "Epoch [290/300], D Loss: -0.1380, G Loss: -0.7138, KL Div: 74.7372\n",
      "Epoch [291/300], D Loss: -0.2384, G Loss: -0.5792, KL Div: 74.5808\n",
      "Epoch [292/300], D Loss: -0.1929, G Loss: -0.5376, KL Div: 74.5664\n",
      "Epoch [293/300], D Loss: -0.2258, G Loss: -0.9424, KL Div: 74.5090\n",
      "Epoch [294/300], D Loss: -0.1397, G Loss: -0.9428, KL Div: 74.4013\n",
      "Epoch [295/300], D Loss: -0.1491, G Loss: -0.9900, KL Div: 74.1392\n",
      "Epoch [296/300], D Loss: -0.2777, G Loss: -0.6696, KL Div: 73.9809\n",
      "Epoch [297/300], D Loss: -0.3983, G Loss: -0.6316, KL Div: 73.5306\n",
      "Epoch [298/300], D Loss: -0.1906, G Loss: -0.8445, KL Div: 73.3518\n",
      "Epoch [299/300], D Loss: -0.1053, G Loss: -0.5020, KL Div: 73.2260\n",
      "Epoch [300/300], D Loss: -0.1639, G Loss: -0.7817, KL Div: 73.2872\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator for the CTGAN.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, discriminator_dim, pac=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        dim = input_dim * pac\n",
    "        self.pac = pac\n",
    "        self.pacdim = dim\n",
    "        seq = []\n",
    "        for item in list(discriminator_dim):\n",
    "            seq += [nn.Linear(dim, item), nn.LeakyReLU(0.2), nn.Dropout(0.5)]\n",
    "            dim = item\n",
    "\n",
    "        seq += [nn.Linear(dim, 1)]\n",
    "        self.seq = nn.Sequential(*seq)\n",
    "\n",
    "    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
    "        \"\"\"Compute the gradient penalty.\"\"\"\n",
    "        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
    "        alpha = alpha.repeat(1, pac, real_data.size(1))\n",
    "        alpha = alpha.view(-1, real_data.size(1))\n",
    "\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "        disc_interpolates = self(interpolates)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=disc_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        gradients_view = gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n",
    "        gradient_penalty = ((gradients_view) ** 2).mean() * lambda_\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"Apply the Discriminator to the `input_`.\"\"\"\n",
    "        assert input_.size()[0] % self.pac == 0\n",
    "        return self.seq(input_.view(-1, self.pacdim))\n",
    "\n",
    "\n",
    "# Bayesian Residual Layer\n",
    "class BayesianResidual(nn.Module):\n",
    "    def __init__(self, i, o):\n",
    "        super(BayesianResidual, self).__init__()\n",
    "        self.fc_mu = nn.Linear(i, o)\n",
    "        self.fc_logvar = nn.Linear(i, o)\n",
    "        self.bn = nn.BatchNorm1d(o)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, input_):\n",
    "        mu = self.fc_mu(input_)\n",
    "        logvar = self.fc_logvar(input_)\n",
    "        out = self.reparameterize(mu, logvar)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return torch.cat([out, input_], dim=1)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        kl = 0\n",
    "        for param_mu, param_logvar in [(self.fc_mu.weight, self.fc_logvar.weight), (self.fc_mu.bias, self.fc_logvar.bias)]:\n",
    "            kl += -0.5 * torch.sum(1 + param_logvar - param_mu.pow(2) - param_logvar.exp())\n",
    "        return kl\n",
    "\n",
    "# Bayesian Generator\n",
    "class BayesianGenerator(nn.Module):\n",
    "    def __init__(self, embedding_dim, generator_dim, data_dim):\n",
    "        super(BayesianGenerator, self).__init__()\n",
    "        dim = embedding_dim\n",
    "        seq = []\n",
    "        for item in list(generator_dim):\n",
    "            seq += [BayesianResidual(dim, item)]\n",
    "            dim += item\n",
    "        self.residuals = nn.ModuleList(seq)\n",
    "        self.final_mu = nn.Linear(dim, data_dim)\n",
    "        self.final_logvar = nn.Linear(dim, data_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        #print(x.shape)\n",
    "        for residual in self.residuals:\n",
    "            x = residual(x)\n",
    "        mu = self.final_mu(x)\n",
    "        logvar = self.final_logvar(x)\n",
    "        data = self.reparameterize(mu, logvar)\n",
    "        return data\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        kl = 0\n",
    "        for residual in self.residuals:\n",
    "            kl += residual.kl_divergence()\n",
    "        for param_mu, param_logvar in [(self.final_mu.weight, self.final_logvar.weight), (self.final_mu.bias, self.final_logvar.bias)]:\n",
    "            kl += -0.5 * torch.sum(1 + param_logvar - param_mu.pow(2) - param_logvar.exp())\n",
    "        return kl\n",
    "\n",
    "# Apply Activate (unchanged)\n",
    "def _apply_activate(data, transformer):\n",
    "    \"\"\"Apply proper activation function to the output of the generator.\"\"\"\n",
    "    data_t = []\n",
    "    st = 0\n",
    "    for column_info in transformer.output_info_list:\n",
    "        for span_info in column_info:\n",
    "            if span_info.activation_fn == 'tanh':\n",
    "                ed = st + span_info.dim\n",
    "                data_t.append(torch.tanh(data[:, st:ed]))\n",
    "                st = ed\n",
    "            elif span_info.activation_fn == 'softmax':\n",
    "                ed = st + span_info.dim\n",
    "                transformed = F.gumbel_softmax(data[:, st:ed], tau=0.2)\n",
    "                data_t.append(transformed)\n",
    "                st = ed\n",
    "            else:\n",
    "                raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n",
    "    return torch.cat(data_t, dim=1)\n",
    "\n",
    "# Cond Loss (unchanged)\n",
    "def cond_loss(data, c, m, transformer):\n",
    "    \"\"\"Compute the cross entropy loss on the fixed discrete column.\"\"\"\n",
    "    loss = []\n",
    "    st = 0\n",
    "    st_c = 0\n",
    "    for column_info in transformer.output_info_list:\n",
    "        for span_info in column_info:\n",
    "            if len(column_info) != 1 or span_info.activation_fn != 'softmax':\n",
    "                # not discrete column\n",
    "                st += span_info.dim\n",
    "            else:\n",
    "                ed = st + span_info.dim\n",
    "                ed_c = st_c + span_info.dim\n",
    "                tmp = F.cross_entropy(\n",
    "                    data[:, st:ed], torch.argmax(c[:, st_c:ed_c], dim=1), reduction='none'\n",
    "                )\n",
    "                loss.append(tmp)\n",
    "                st = ed\n",
    "                st_c = ed_c\n",
    "    loss = torch.stack(loss, dim=1)\n",
    "    return (loss * m).sum() / data.size()[0]\n",
    "\n",
    "def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n",
    "    return F.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n",
    "\n",
    "batch_size = 500\n",
    "embedding_dim = 128\n",
    "generator_dim = (256, 256)\n",
    "discriminator_dim = (256, 256)\n",
    "data_dim = transformer.output_dimensions\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = BayesianGenerator(embedding_dim + data_sampler.dim_cond_vec(), generator_dim, data_dim).to(device)\n",
    "discriminator = Discriminator(data_dim + data_sampler.dim_cond_vec(), discriminator_dim).to(device)\n",
    "\n",
    "optimizerG = torch.optim.Adam(\n",
    "            generator.parameters(),\n",
    "            lr=2e-4,\n",
    "            betas=(0.5, 0.9),\n",
    "            weight_decay=1e-6\n",
    "        )\n",
    "\n",
    "optimizerD = torch.optim.Adam(\n",
    "            discriminator.parameters(),\n",
    "            lr=2e-4,\n",
    "            betas=(0.5, 0.9),\n",
    "            weight_decay=1e-6\n",
    "        )\n",
    "\n",
    "mean = torch.zeros(batch_size, embedding_dim, device=device)\n",
    "std = mean + 1\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 300\n",
    "discriminator_steps = 1\n",
    "steps_per_epoch = max(len(train_data) // batch_size, 1)\n",
    "kl_weight = 0.005  # Added for Bayesian regularization\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for id_ in range(steps_per_epoch):\n",
    "        # Discriminator Training\n",
    "        for n in range(discriminator_steps):\n",
    "            fakez = torch.normal(mean=mean, std=std).to(device)\n",
    "\n",
    "            condvec = data_sampler.sample_condvec(batch_size)\n",
    "            if condvec is None:\n",
    "                c1, m1, col, opt = None, None, None, None\n",
    "                real = data_sampler.sample_data(train_data, batch_size, col, opt)\n",
    "            else:\n",
    "                c1, m1, col, opt = condvec\n",
    "                c1 = torch.from_numpy(c1).to(device)\n",
    "                m1 = torch.from_numpy(m1).to(device)\n",
    "                #print(fakez.shape, c1.shape)\n",
    "                fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                perm = np.arange(batch_size)\n",
    "                np.random.shuffle(perm)\n",
    "                real = data_sampler.sample_data(train_data, batch_size, col[perm], opt[perm])\n",
    "                c2 = c1[perm]\n",
    "            fake = generator(fakez)\n",
    "            fakeact = _apply_activate(fake, transformer=transformer)\n",
    "\n",
    "            real = torch.from_numpy(real.astype('float32')).to(device)\n",
    "\n",
    "            if c1 is not None:\n",
    "                fake_cat = torch.cat([fakeact, c1], dim=1)\n",
    "                real_cat = torch.cat([real, c2], dim=1)\n",
    "            else:\n",
    "                real_cat = real\n",
    "                fake_cat = fakeact\n",
    "\n",
    "            y_fake = discriminator(fake_cat)\n",
    "            y_real = discriminator(real_cat)\n",
    "\n",
    "            pen = discriminator.calc_gradient_penalty(real_cat, fake_cat, device, pac=10)\n",
    "            loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n",
    "\n",
    "            optimizerD.zero_grad(set_to_none=False)\n",
    "            pen.backward(retain_graph=True)\n",
    "            loss_d.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "        # Generator Training\n",
    "        fakez = torch.normal(mean=mean, std=std).to(device)\n",
    "        condvec = data_sampler.sample_condvec(batch_size)\n",
    "\n",
    "        if condvec is None:\n",
    "            c1, m1, col, opt = None, None, None, None\n",
    "        else:\n",
    "            c1, m1, col, opt = condvec\n",
    "            c1 = torch.from_numpy(c1).to(device)\n",
    "            m1 = torch.from_numpy(m1).to(device)\n",
    "            fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "        fake = generator(fakez)\n",
    "        fakeact = _apply_activate(fake, transformer=transformer)\n",
    "\n",
    "        if c1 is not None:\n",
    "            y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n",
    "        else:\n",
    "            y_fake = discriminator(fakeact)\n",
    "\n",
    "        if condvec is None:\n",
    "            cross_entropy = 0\n",
    "        else:\n",
    "            cross_entropy = cond_loss(fake, c1, m1, transformer=transformer)\n",
    "\n",
    "        kl_div = generator.kl_divergence()  # Bayesian KL divergence term\n",
    "        loss_g = -torch.mean(y_fake) + cross_entropy + kl_weight * kl_div\n",
    "\n",
    "        optimizerG.zero_grad(set_to_none=False)\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    generator_loss = loss_g.detach().cpu().item()\n",
    "    discriminator_loss = loss_d.detach().cpu().item()\n",
    "    kl_div_value = kl_div.detach().cpu().item()\n",
    "\n",
    "    print(f\"Epoch [{i+1}/{num_epochs}], D Loss: {discriminator_loss:.4f}, G Loss: {generator_loss:.4f}, KL Div: {kl_div_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c085125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output variance: 3.4168\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>2603</td>\n",
       "      <td>70</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>3</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass education  education-num      marital-status  \\\n",
       "0   43   Private   HS-grad              9  Married-civ-spouse   \n",
       "1   35   Private   HS-grad              9            Divorced   \n",
       "2   34   Private   HS-grad              9            Divorced   \n",
       "3   40   Private   7th-8th              3  Married-civ-spouse   \n",
       "\n",
       "          occupation   relationship   race     sex  capital-gain  \\\n",
       "0  Machine-op-inspct           Wife  White  Female             0   \n",
       "1     Prof-specialty        Husband  White  Female             0   \n",
       "2   Transport-moving  Not-in-family  White  Female             0   \n",
       "3    Protective-serv        Husband  White    Male             0   \n",
       "\n",
       "   capital-loss  hours-per-week native-country  \n",
       "0             0              40  United-States  \n",
       "1          2603              70  United-States  \n",
       "2             0              60  United-States  \n",
       "3             0              40  United-States  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample(n, transformer, generator, batch_size, condition_column=None, condition_value=None):\n",
    "    \"\"\"Sample data similar to the training data.\n",
    "\n",
    "    Choosing a condition_column and condition_value will increase the probability of the\n",
    "    discrete condition_value happening in the condition_column.\n",
    "\n",
    "    Args:\n",
    "        n (int):\n",
    "            Number of rows to sample.\n",
    "        condition_column (string):\n",
    "            Name of a discrete column.\n",
    "        condition_value (string):\n",
    "            Name of the category in the condition_column which we wish to increase the\n",
    "            probability of happening.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if condition_column is not None and condition_value is not None:\n",
    "        condition_info = transformer.convert_column_name_value_to_id(\n",
    "            condition_column, condition_value\n",
    "        )\n",
    "        global_condition_vec = data_sampler.generate_cond_from_condition_column_info(\n",
    "            condition_info, batch_size\n",
    "        )\n",
    "    else:\n",
    "        global_condition_vec = None\n",
    "\n",
    "    steps = n // batch_size + 1\n",
    "    data = []\n",
    "    for i in range(steps):\n",
    "        mean = torch.zeros(batch_size, embedding_dim)\n",
    "        std = mean + 1\n",
    "        fakez = torch.normal(mean=mean, std=std).to(device)\n",
    "\n",
    "        if global_condition_vec is not None:\n",
    "            condvec = global_condition_vec.copy()\n",
    "        else:\n",
    "            condvec = data_sampler.sample_original_condvec(batch_size)\n",
    "\n",
    "        if condvec is None:\n",
    "            pass\n",
    "        else:\n",
    "            c1 = condvec\n",
    "            c1 = torch.from_numpy(c1).to(device)\n",
    "            fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = [generator(fakez).cpu().numpy() for _ in range(10)]\n",
    "            variance = np.var(outputs, axis=0)\n",
    "            print(f\"Output variance: {variance.mean():.4f}\")\n",
    "            \n",
    "        fake = generator(fakez)\n",
    "        fakeact = _apply_activate(fake, transformer=transformer)\n",
    "        data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    data = data[:n]\n",
    "\n",
    "    return transformer.inverse_transform(data)\n",
    "\n",
    "sample(4, transformer=transformer, generator=generator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9a7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ydata-profiling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
