{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1cb11406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a0bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=6)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "loader = DataLoader(TensorDataset(X), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be0089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.mu = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "        self.log_sigma = nn.Parameter(torch.full((out_features, in_features), -3.0))\n",
    "        self.bias_mu = nn.Parameter(torch.zeros(out_features))\n",
    "        self.bias_log_sigma = nn.Parameter(torch.full((out_features,), -3.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.mu + torch.exp(self.log_sigma) * torch.randn_like(self.mu)\n",
    "        bias = self.bias_mu + torch.exp(self.bias_log_sigma) * torch.randn_like(self.bias_mu)\n",
    "        return x @ weight.t() + bias\n",
    "\n",
    "    def kl_loss(self):\n",
    "        return 0.5 * torch.sum(\n",
    "            torch.exp(2 * self.log_sigma) + self.mu**2 - 1 - 2 * self.log_sigma\n",
    "        ) + 0.5 * torch.sum(\n",
    "            torch.exp(2 * self.bias_log_sigma) + self.bias_mu**2 - 1 - 2 * self.bias_log_sigma\n",
    "        )\n",
    "\n",
    "class BayesianGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, data_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = BayesianLinear(z_dim, hidden_dim)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, data_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc1(z))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def kl_loss(self):\n",
    "        return self.fc1.kl_loss() + self.fc2.kl_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa2d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "644e5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | D Loss: 1.4758 | G Loss: 0.6935 | KL: 2.1879\n",
      "Epoch 2 | D Loss: 1.4853 | G Loss: 0.6669 | KL: 2.1852\n",
      "Epoch 3 | D Loss: 1.4666 | G Loss: 0.6463 | KL: 2.1825\n",
      "Epoch 4 | D Loss: 1.4217 | G Loss: 0.6662 | KL: 2.1799\n",
      "Epoch 5 | D Loss: 1.4226 | G Loss: 0.6096 | KL: 2.1772\n",
      "Epoch 6 | D Loss: 1.3995 | G Loss: 0.6309 | KL: 2.1746\n",
      "Epoch 7 | D Loss: 1.3489 | G Loss: 0.6312 | KL: 2.1720\n",
      "Epoch 8 | D Loss: 1.3164 | G Loss: 0.6255 | KL: 2.1693\n",
      "Epoch 9 | D Loss: 1.3158 | G Loss: 0.6318 | KL: 2.1667\n",
      "Epoch 10 | D Loss: 1.3602 | G Loss: 0.6476 | KL: 2.1641\n",
      "Epoch 11 | D Loss: 1.3732 | G Loss: 0.6442 | KL: 2.1615\n",
      "Epoch 12 | D Loss: 1.3546 | G Loss: 0.6293 | KL: 2.1589\n",
      "Epoch 13 | D Loss: 1.2750 | G Loss: 0.6236 | KL: 2.1563\n",
      "Epoch 14 | D Loss: 1.2696 | G Loss: 0.6252 | KL: 2.1537\n",
      "Epoch 15 | D Loss: 1.2408 | G Loss: 0.6626 | KL: 2.1511\n",
      "Epoch 16 | D Loss: 1.2251 | G Loss: 0.6433 | KL: 2.1486\n",
      "Epoch 17 | D Loss: 1.1852 | G Loss: 0.6276 | KL: 2.1460\n",
      "Epoch 18 | D Loss: 1.2307 | G Loss: 0.6279 | KL: 2.1434\n",
      "Epoch 19 | D Loss: 1.1945 | G Loss: 0.6727 | KL: 2.1409\n",
      "Epoch 20 | D Loss: 1.1841 | G Loss: 0.6489 | KL: 2.1383\n",
      "Epoch 21 | D Loss: 1.1233 | G Loss: 0.6376 | KL: 2.1358\n",
      "Epoch 22 | D Loss: 1.1503 | G Loss: 0.6642 | KL: 2.1332\n",
      "Epoch 23 | D Loss: 1.1196 | G Loss: 0.6746 | KL: 2.1307\n",
      "Epoch 24 | D Loss: 1.1509 | G Loss: 0.6466 | KL: 2.1281\n",
      "Epoch 25 | D Loss: 1.0930 | G Loss: 0.6878 | KL: 2.1256\n",
      "Epoch 26 | D Loss: 1.0932 | G Loss: 0.6984 | KL: 2.1231\n",
      "Epoch 27 | D Loss: 1.0826 | G Loss: 0.6693 | KL: 2.1206\n",
      "Epoch 28 | D Loss: 1.0973 | G Loss: 0.6929 | KL: 2.1180\n",
      "Epoch 29 | D Loss: 1.1111 | G Loss: 0.6543 | KL: 2.1155\n",
      "Epoch 30 | D Loss: 1.0709 | G Loss: 0.7243 | KL: 2.1130\n",
      "Epoch 31 | D Loss: 1.0942 | G Loss: 0.7467 | KL: 2.1105\n",
      "Epoch 32 | D Loss: 1.0712 | G Loss: 0.7193 | KL: 2.1080\n",
      "Epoch 33 | D Loss: 1.1144 | G Loss: 0.7592 | KL: 2.1055\n",
      "Epoch 34 | D Loss: 1.0575 | G Loss: 0.7471 | KL: 2.1030\n",
      "Epoch 35 | D Loss: 1.0344 | G Loss: 0.7763 | KL: 2.1004\n",
      "Epoch 36 | D Loss: 0.9919 | G Loss: 0.7900 | KL: 2.0979\n",
      "Epoch 37 | D Loss: 0.9918 | G Loss: 0.8217 | KL: 2.0954\n",
      "Epoch 38 | D Loss: 1.0082 | G Loss: 0.8463 | KL: 2.0929\n",
      "Epoch 39 | D Loss: 0.9458 | G Loss: 0.8529 | KL: 2.0903\n",
      "Epoch 40 | D Loss: 0.9755 | G Loss: 0.8342 | KL: 2.0878\n",
      "Epoch 41 | D Loss: 0.9771 | G Loss: 0.8450 | KL: 2.0853\n",
      "Epoch 42 | D Loss: 0.8709 | G Loss: 0.8690 | KL: 2.0828\n",
      "Epoch 43 | D Loss: 0.9736 | G Loss: 0.8326 | KL: 2.0803\n",
      "Epoch 44 | D Loss: 0.9275 | G Loss: 0.8614 | KL: 2.0778\n",
      "Epoch 45 | D Loss: 0.9678 | G Loss: 0.9120 | KL: 2.0752\n",
      "Epoch 46 | D Loss: 0.9609 | G Loss: 0.8791 | KL: 2.0727\n",
      "Epoch 47 | D Loss: 0.9474 | G Loss: 0.8922 | KL: 2.0703\n",
      "Epoch 48 | D Loss: 0.8228 | G Loss: 0.9008 | KL: 2.0678\n",
      "Epoch 49 | D Loss: 0.9678 | G Loss: 0.8749 | KL: 2.0653\n",
      "Epoch 50 | D Loss: 0.9357 | G Loss: 0.8825 | KL: 2.0628\n",
      "Epoch 51 | D Loss: 0.8984 | G Loss: 0.8451 | KL: 2.0603\n",
      "Epoch 52 | D Loss: 0.8619 | G Loss: 0.8832 | KL: 2.0579\n",
      "Epoch 53 | D Loss: 0.9842 | G Loss: 0.7850 | KL: 2.0555\n",
      "Epoch 54 | D Loss: 1.0107 | G Loss: 0.7706 | KL: 2.0530\n",
      "Epoch 55 | D Loss: 0.9807 | G Loss: 0.8819 | KL: 2.0506\n",
      "Epoch 56 | D Loss: 1.0507 | G Loss: 0.7625 | KL: 2.0482\n",
      "Epoch 57 | D Loss: 0.9891 | G Loss: 0.7892 | KL: 2.0458\n",
      "Epoch 58 | D Loss: 1.0575 | G Loss: 0.7498 | KL: 2.0435\n",
      "Epoch 59 | D Loss: 0.9324 | G Loss: 0.8219 | KL: 2.0411\n",
      "Epoch 60 | D Loss: 1.0708 | G Loss: 0.7224 | KL: 2.0387\n",
      "Epoch 61 | D Loss: 1.0796 | G Loss: 0.7809 | KL: 2.0364\n",
      "Epoch 62 | D Loss: 1.0503 | G Loss: 0.7330 | KL: 2.0340\n",
      "Epoch 63 | D Loss: 0.9812 | G Loss: 0.7533 | KL: 2.0317\n",
      "Epoch 64 | D Loss: 1.0809 | G Loss: 0.7420 | KL: 2.0294\n",
      "Epoch 65 | D Loss: 0.9445 | G Loss: 0.7735 | KL: 2.0270\n",
      "Epoch 66 | D Loss: 1.0987 | G Loss: 0.7185 | KL: 2.0247\n",
      "Epoch 67 | D Loss: 0.9544 | G Loss: 0.7332 | KL: 2.0224\n",
      "Epoch 68 | D Loss: 0.8821 | G Loss: 0.7336 | KL: 2.0200\n",
      "Epoch 69 | D Loss: 0.9661 | G Loss: 0.7554 | KL: 2.0177\n",
      "Epoch 70 | D Loss: 0.9366 | G Loss: 0.7589 | KL: 2.0154\n",
      "Epoch 71 | D Loss: 0.9279 | G Loss: 0.7912 | KL: 2.0131\n",
      "Epoch 72 | D Loss: 0.9224 | G Loss: 0.7909 | KL: 2.0108\n",
      "Epoch 73 | D Loss: 0.8667 | G Loss: 0.7806 | KL: 2.0085\n",
      "Epoch 74 | D Loss: 0.9982 | G Loss: 0.8260 | KL: 2.0062\n",
      "Epoch 75 | D Loss: 0.9768 | G Loss: 0.8114 | KL: 2.0039\n",
      "Epoch 76 | D Loss: 0.9006 | G Loss: 0.8248 | KL: 2.0017\n",
      "Epoch 77 | D Loss: 1.0554 | G Loss: 0.8225 | KL: 1.9994\n",
      "Epoch 78 | D Loss: 1.0289 | G Loss: 0.7979 | KL: 1.9972\n",
      "Epoch 79 | D Loss: 0.9740 | G Loss: 0.8878 | KL: 1.9950\n",
      "Epoch 80 | D Loss: 0.8751 | G Loss: 0.9176 | KL: 1.9928\n",
      "Epoch 81 | D Loss: 0.7132 | G Loss: 0.8726 | KL: 1.9906\n",
      "Epoch 82 | D Loss: 0.8554 | G Loss: 0.9257 | KL: 1.9884\n",
      "Epoch 83 | D Loss: 0.8963 | G Loss: 0.9621 | KL: 1.9863\n",
      "Epoch 84 | D Loss: 0.9745 | G Loss: 0.9961 | KL: 1.9841\n",
      "Epoch 85 | D Loss: 0.8664 | G Loss: 0.9756 | KL: 1.9819\n",
      "Epoch 86 | D Loss: 0.8882 | G Loss: 1.1788 | KL: 1.9798\n",
      "Epoch 87 | D Loss: 0.8347 | G Loss: 1.1175 | KL: 1.9777\n",
      "Epoch 88 | D Loss: 0.8910 | G Loss: 1.0479 | KL: 1.9755\n",
      "Epoch 89 | D Loss: 0.8911 | G Loss: 1.0547 | KL: 1.9734\n",
      "Epoch 90 | D Loss: 0.8509 | G Loss: 1.0601 | KL: 1.9713\n",
      "Epoch 91 | D Loss: 0.7785 | G Loss: 1.0925 | KL: 1.9692\n",
      "Epoch 92 | D Loss: 0.7134 | G Loss: 1.0186 | KL: 1.9671\n",
      "Epoch 93 | D Loss: 0.8836 | G Loss: 1.2158 | KL: 1.9650\n",
      "Epoch 94 | D Loss: 0.9128 | G Loss: 0.9424 | KL: 1.9630\n",
      "Epoch 95 | D Loss: 0.8482 | G Loss: 1.0888 | KL: 1.9610\n",
      "Epoch 96 | D Loss: 0.8189 | G Loss: 1.0806 | KL: 1.9590\n",
      "Epoch 97 | D Loss: 0.8617 | G Loss: 0.9178 | KL: 1.9570\n",
      "Epoch 98 | D Loss: 0.8739 | G Loss: 0.9912 | KL: 1.9550\n",
      "Epoch 99 | D Loss: 0.9954 | G Loss: 1.0919 | KL: 1.9530\n",
      "Epoch 100 | D Loss: 1.0225 | G Loss: 1.0225 | KL: 1.9511\n"
     ]
    }
   ],
   "source": [
    "z_dim = 16\n",
    "gen = BayesianGenerator(z_dim, 32, X.shape[1])\n",
    "disc = Discriminator(X.shape[1], 32)\n",
    "\n",
    "g_opt = torch.optim.Adam(gen.parameters(), lr=2e-4)\n",
    "d_opt = torch.optim.Adam(disc.parameters(), lr=2e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for real_data, in loader:\n",
    "        batch_size = real_data.size(0)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, z_dim)\n",
    "        fake_data = gen(z).detach()\n",
    "        d_real = disc(real_data)\n",
    "        d_fake = disc(fake_data)\n",
    "        d_loss = -torch.mean(torch.log(d_real + 1e-8) + torch.log(1 - d_fake + 1e-8))\n",
    "\n",
    "        d_opt.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_opt.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, z_dim)\n",
    "        fake_data = gen(z)\n",
    "        g_preds = disc(fake_data)\n",
    "        g_loss = -torch.mean(torch.log(g_preds + 1e-8))\n",
    "\n",
    "        kl = gen.kl_loss() / len(loader.dataset)\n",
    "        total_g_loss = g_loss + kl\n",
    "\n",
    "        g_opt.zero_grad()\n",
    "        total_g_loss.backward()\n",
    "        g_opt.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f} | KL: {kl.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e8a89cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | D Loss: 1.3185 | G Loss: 0.5994\n",
      "Epoch 1 | D Loss: 1.1703 | G Loss: 0.4114\n",
      "Epoch 2 | D Loss: 1.1065 | G Loss: 0.4029\n",
      "Epoch 3 | D Loss: 1.2548 | G Loss: 0.6963\n",
      "Epoch 4 | D Loss: 1.2957 | G Loss: 0.3425\n",
      "Epoch 5 | D Loss: 1.4968 | G Loss: 0.5220\n",
      "Epoch 6 | D Loss: 0.9136 | G Loss: 0.0736\n",
      "Epoch 7 | D Loss: 1.3639 | G Loss: 0.3352\n",
      "Epoch 8 | D Loss: 0.9493 | G Loss: 0.0696\n",
      "Epoch 9 | D Loss: 0.8289 | G Loss: 0.0952\n",
      "Epoch 10 | D Loss: 0.8122 | G Loss: 0.0018\n",
      "Epoch 11 | D Loss: 0.8407 | G Loss: 0.0764\n",
      "Epoch 12 | D Loss: 1.1089 | G Loss: 0.0519\n",
      "Epoch 13 | D Loss: 0.8390 | G Loss: 0.0180\n",
      "Epoch 14 | D Loss: 1.0571 | G Loss: 0.0127\n",
      "Epoch 15 | D Loss: 1.0992 | G Loss: 2.5099\n",
      "Epoch 16 | D Loss: 0.8154 | G Loss: 0.0000\n",
      "Epoch 17 | D Loss: 0.8250 | G Loss: 0.0000\n",
      "Epoch 18 | D Loss: 0.8223 | G Loss: 0.0000\n",
      "Epoch 19 | D Loss: 0.8103 | G Loss: 0.0000\n",
      "Epoch 20 | D Loss: 0.8103 | G Loss: 0.0002\n",
      "Epoch 21 | D Loss: 0.9751 | G Loss: 0.0001\n",
      "Epoch 22 | D Loss: 0.7716 | G Loss: 0.0000\n",
      "Epoch 23 | D Loss: 0.9123 | G Loss: 0.0000\n",
      "Epoch 24 | D Loss: 0.7833 | G Loss: 0.0000\n",
      "Epoch 25 | D Loss: 0.7590 | G Loss: 0.0000\n",
      "Epoch 26 | D Loss: 0.7826 | G Loss: 0.0000\n",
      "Epoch 27 | D Loss: 0.7488 | G Loss: 0.0000\n",
      "Epoch 28 | D Loss: 0.8074 | G Loss: 0.0000\n",
      "Epoch 29 | D Loss: 0.7360 | G Loss: 0.0000\n",
      "Epoch 30 | D Loss: 0.7139 | G Loss: 0.0000\n",
      "Epoch 31 | D Loss: 0.7313 | G Loss: 0.0000\n",
      "Epoch 32 | D Loss: 0.7413 | G Loss: 0.0000\n",
      "Epoch 33 | D Loss: 0.7415 | G Loss: 0.0000\n",
      "Epoch 34 | D Loss: 0.7157 | G Loss: 0.0000\n",
      "Epoch 35 | D Loss: 0.7060 | G Loss: 0.0000\n",
      "Epoch 36 | D Loss: 0.6964 | G Loss: 0.0000\n",
      "Epoch 37 | D Loss: 0.6884 | G Loss: 0.0000\n",
      "Epoch 38 | D Loss: 0.6850 | G Loss: 0.0000\n",
      "Epoch 39 | D Loss: 0.6910 | G Loss: 0.0000\n",
      "Epoch 40 | D Loss: 0.6696 | G Loss: 0.0000\n",
      "Epoch 41 | D Loss: 0.6810 | G Loss: 0.0000\n",
      "Epoch 42 | D Loss: 0.6580 | G Loss: 0.0000\n",
      "Epoch 43 | D Loss: 0.6703 | G Loss: 0.0000\n",
      "Epoch 44 | D Loss: 0.6524 | G Loss: 0.0000\n",
      "Epoch 45 | D Loss: 0.6315 | G Loss: 0.0000\n",
      "Epoch 46 | D Loss: 0.7088 | G Loss: 0.0000\n",
      "Epoch 47 | D Loss: 0.6369 | G Loss: 0.0000\n",
      "Epoch 48 | D Loss: 0.6193 | G Loss: 0.0000\n",
      "Epoch 49 | D Loss: 0.6320 | G Loss: 0.0000\n",
      "Epoch 50 | D Loss: 0.6383 | G Loss: 0.0000\n",
      "Epoch 51 | D Loss: 0.6638 | G Loss: 0.0003\n",
      "Epoch 52 | D Loss: 0.6292 | G Loss: 0.0006\n",
      "Epoch 53 | D Loss: 0.6340 | G Loss: 0.0000\n",
      "Epoch 54 | D Loss: 0.6278 | G Loss: 0.0000\n",
      "Epoch 55 | D Loss: 0.7102 | G Loss: 0.0004\n",
      "Epoch 56 | D Loss: 0.6564 | G Loss: 0.0000\n",
      "Epoch 57 | D Loss: 0.6365 | G Loss: 0.0013\n",
      "Epoch 58 | D Loss: 0.6361 | G Loss: 0.0000\n",
      "Epoch 59 | D Loss: 0.6025 | G Loss: 0.0000\n",
      "Epoch 60 | D Loss: 0.6171 | G Loss: 0.0000\n",
      "Epoch 61 | D Loss: 0.6106 | G Loss: 0.0000\n",
      "Epoch 62 | D Loss: 0.6169 | G Loss: 0.0000\n",
      "Epoch 63 | D Loss: 0.6235 | G Loss: 0.0000\n",
      "Epoch 64 | D Loss: 0.6290 | G Loss: 0.0000\n",
      "Epoch 65 | D Loss: 0.6112 | G Loss: 0.0000\n",
      "Epoch 66 | D Loss: 0.6119 | G Loss: 0.0000\n",
      "Epoch 67 | D Loss: 0.6337 | G Loss: 0.0000\n",
      "Epoch 68 | D Loss: 0.5920 | G Loss: 0.0877\n",
      "Epoch 69 | D Loss: 0.6161 | G Loss: 0.0000\n",
      "Epoch 70 | D Loss: 0.6048 | G Loss: 0.0029\n",
      "Epoch 71 | D Loss: 0.5879 | G Loss: 0.0000\n",
      "Epoch 72 | D Loss: 0.6028 | G Loss: 0.0000\n",
      "Epoch 73 | D Loss: 0.6072 | G Loss: 0.0000\n",
      "Epoch 74 | D Loss: 0.6153 | G Loss: 0.0000\n",
      "Epoch 75 | D Loss: 0.5823 | G Loss: 0.0000\n",
      "Epoch 76 | D Loss: 0.5853 | G Loss: 0.0000\n",
      "Epoch 77 | D Loss: 0.5923 | G Loss: 0.0005\n",
      "Epoch 78 | D Loss: 0.5779 | G Loss: 0.0000\n",
      "Epoch 79 | D Loss: 0.5887 | G Loss: 0.0000\n",
      "Epoch 80 | D Loss: 0.5673 | G Loss: 0.0000\n",
      "Epoch 81 | D Loss: 0.5687 | G Loss: 0.0000\n",
      "Epoch 82 | D Loss: 0.5696 | G Loss: 0.0000\n",
      "Epoch 83 | D Loss: 0.5980 | G Loss: 0.0000\n",
      "Epoch 84 | D Loss: 0.5715 | G Loss: 0.0000\n",
      "Epoch 85 | D Loss: 0.5924 | G Loss: 0.0000\n",
      "Epoch 86 | D Loss: 0.5642 | G Loss: 0.0000\n",
      "Epoch 87 | D Loss: 0.6147 | G Loss: 0.0000\n",
      "Epoch 88 | D Loss: 0.5789 | G Loss: 0.0000\n",
      "Epoch 89 | D Loss: 0.5777 | G Loss: 0.0000\n",
      "Epoch 90 | D Loss: 0.9218 | G Loss: 0.0000\n",
      "Epoch 91 | D Loss: 0.5758 | G Loss: 0.0000\n",
      "Epoch 92 | D Loss: 0.5701 | G Loss: 0.0000\n",
      "Epoch 93 | D Loss: 1.0360 | G Loss: 0.0184\n",
      "Epoch 94 | D Loss: 0.5718 | G Loss: 0.0000\n",
      "Epoch 95 | D Loss: 0.5648 | G Loss: 0.0000\n",
      "Epoch 96 | D Loss: 0.5724 | G Loss: 0.0000\n",
      "Epoch 97 | D Loss: 1.0207 | G Loss: 0.0000\n",
      "Epoch 98 | D Loss: 0.5595 | G Loss: 0.0000\n",
      "Epoch 99 | D Loss: 0.5568 | G Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------ Network Definitions ------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        #self.fc1 = BayesianLinear(z_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        #self.fc2 = BayesianLinear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc1(z))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def sghmc_update(params, grads, momentum, step_size, friction, noise_std):\n",
    "    for p, g, m in zip(params, grads, momentum):\n",
    "        if g is None: continue\n",
    "        m.data = (1 - friction) * m.data + step_size * g + noise_std * torch.randn_like(p)\n",
    "        p.data -= m.data\n",
    "\n",
    "# ------------------ Training Loop ------------------\n",
    "def train_bayesian_gan_sghmc(real_data_loader, z_dim=16, hidden_dim=32, output_dim=X.shape[1], \n",
    "                              num_epochs=100, step_size=1e-3, friction=0.05):\n",
    "    G = Generator(z_dim, hidden_dim, output_dim)\n",
    "    D = Discriminator(output_dim, hidden_dim)\n",
    "    d_optimizer = torch.optim.Adam(D.parameters(), lr=1e-4)\n",
    "\n",
    "    # SGHMC state for Generator\n",
    "    g_params = list(G.parameters())\n",
    "    g_momentum = [torch.zeros_like(p) for p in g_params]\n",
    "    noise_std = torch.sqrt(torch.tensor(2 * step_size * friction))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_batch, in real_data_loader:\n",
    "            real_batch = real_batch.float()\n",
    "            batch_size = real_batch.size(0)\n",
    "\n",
    "            # ------------------ Train Discriminator ------------------\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = G(z).detach()\n",
    "            \n",
    "            d_real = D(real_batch)\n",
    "            d_fake = D(fake_data)\n",
    "            d_loss = -torch.mean(torch.log(d_real + 1e-8) + torch.log(1 - d_fake + 1e-8))\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # ------------------ SGHMC Generator Update ------------------\n",
    "            real_labels = torch.zeros(batch_size, 1)\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = G(z)\n",
    "            g_loss = F.binary_cross_entropy(D(fake_data), real_labels)\n",
    "\n",
    "            G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_grads = [p.grad for p in g_params]\n",
    "            sghmc_update(g_params, g_grads, g_momentum, step_size, friction, noise_std)\n",
    "\n",
    "        print(f\"Epoch {epoch} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    return G\n",
    "\n",
    "train_bayesian_gan_sghmc(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118f8ba",
   "metadata": {},
   "source": [
    "## Bayesian CTGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e34b3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a464e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        #self.fc1 = BayesianLinear(z_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        #self.fc2 = BayesianLinear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc1(z))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa497faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n",
    "    \"\"\"Deals with the instability of the gumbel_softmax for older versions of torch.\n",
    "\n",
    "    For more details about the issue:\n",
    "    https://drive.google.com/file/d/1AA5wPfZ1kquaRtVruCd6BiYZGcDeNxyP/view?usp=sharing\n",
    "\n",
    "    Args:\n",
    "        logits […, num_features]:\n",
    "            Unnormalized log probabilities\n",
    "        tau:\n",
    "            Non-negative scalar temperature\n",
    "        hard (bool):\n",
    "            If True, the returned samples will be discretized as one-hot vectors,\n",
    "            but will be differentiated as if it is the soft sample in autograd\n",
    "        dim (int):\n",
    "            A dimension along which softmax will be computed. Default: -1.\n",
    "\n",
    "    Returns:\n",
    "        Sampled tensor of same shape as logits from the Gumbel-Softmax distribution.\n",
    "    \"\"\"\n",
    "    for _ in range(10):\n",
    "        transformed = F.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n",
    "        if not torch.isnan(transformed).any():\n",
    "            return transformed\n",
    "\n",
    "    raise ValueError('gumbel_softmax returning NaN.')\n",
    "\n",
    "def apply_activate(data, transformer):\n",
    "    \"\"\"Apply proper activation function to the output of the generator.\"\"\"\n",
    "    data_t = []\n",
    "    st = 0\n",
    "    for column_info in transformer.output_info_list:\n",
    "        for span_info in column_info:\n",
    "            if span_info.activation_fn == 'tanh':\n",
    "                ed = st + span_info.dim\n",
    "                data_t.append(torch.tanh(data[:, st:ed]))\n",
    "                st = ed\n",
    "            elif span_info.activation_fn == 'softmax':\n",
    "                ed = st + span_info.dim\n",
    "                transformed = gumbel_softmax(data[:, st:ed], tau=0.2)\n",
    "                data_t.append(transformed)\n",
    "                st = ed\n",
    "            else:\n",
    "                raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n",
    "\n",
    "    return torch.cat(data_t, dim=1)\n",
    "\n",
    "def cond_loss(data, c, m, transformer):\n",
    "    \"\"\"Compute the cross entropy loss on the fixed discrete column.\"\"\"\n",
    "    loss = []\n",
    "    st = 0\n",
    "    st_c = 0\n",
    "    for column_info in transformer.output_info_list:\n",
    "        for span_info in column_info:\n",
    "            if len(column_info) != 1 or span_info.activation_fn != 'softmax':\n",
    "                # not discrete column\n",
    "                st += span_info.dim\n",
    "            else:\n",
    "                ed = st + span_info.dim\n",
    "                ed_c = st_c + span_info.dim\n",
    "                tmp = F.cross_entropy(\n",
    "                    data[:, st:ed], torch.argmax(c[:, st_c:ed_c], dim=1), reduction='none'\n",
    "                )\n",
    "                loss.append(tmp)\n",
    "                st = ed\n",
    "                st_c = ed_c\n",
    "\n",
    "    loss = torch.stack(loss, dim=1)  # noqa: PD013\n",
    "\n",
    "    return (loss * m).sum() / data.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3153146e",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a759fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7d18911",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult.data.features\n",
    "y = adult.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324c873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt  education  education-num  \\\n",
       "0       39         State-gov   77516  Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2       38           Private  215646    HS-grad              9   \n",
       "3       53           Private  234721       11th              7   \n",
       "4       28           Private  338409  Bachelors             13   \n",
       "...    ...               ...     ...        ...            ...   \n",
       "48837   39           Private  215419  Bachelors             13   \n",
       "48838   64               NaN  321403    HS-grad              9   \n",
       "48839   38           Private  374983  Bachelors             13   \n",
       "48840   44           Private   83891  Bachelors             13   \n",
       "48841   35      Self-emp-inc  182148  Bachelors             13   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           Never-married       Adm-clerical   Not-in-family   \n",
       "1      Married-civ-spouse    Exec-managerial         Husband   \n",
       "2                Divorced  Handlers-cleaners   Not-in-family   \n",
       "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
       "4      Married-civ-spouse     Prof-specialty            Wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            Divorced     Prof-specialty   Not-in-family   \n",
       "48838             Widowed                NaN  Other-relative   \n",
       "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
       "48840            Divorced       Adm-clerical       Own-child   \n",
       "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   White    Male          2174             0              40   \n",
       "1                   White    Male             0             0              13   \n",
       "2                   White    Male             0             0              40   \n",
       "3                   Black    Male             0             0              40   \n",
       "4                   Black  Female             0             0              40   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               White  Female             0             0              36   \n",
       "48838               Black    Male             0             0              40   \n",
       "48839               White    Male             0             0              50   \n",
       "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
       "48841               White    Male             0             0              60   \n",
       "\n",
       "      native-country  \n",
       "0      United-States  \n",
       "1      United-States  \n",
       "2      United-States  \n",
       "3      United-States  \n",
       "4               Cuba  \n",
       "...              ...  \n",
       "48837  United-States  \n",
       "48838  United-States  \n",
       "48839  United-States  \n",
       "48840  United-States  \n",
       "48841  United-States  \n",
       "\n",
       "[48842 rows x 14 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c29fd5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age size: 74\n",
      "workclass size: 10\n",
      "fnlwgt size: 28523\n",
      "education size: 16\n",
      "education-num size: 16\n",
      "marital-status size: 7\n",
      "occupation size: 16\n",
      "relationship size: 6\n",
      "race size: 5\n",
      "sex size: 2\n",
      "capital-gain size: 123\n",
      "capital-loss size: 99\n",
      "hours-per-week size: 96\n",
      "native-country size: 43\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: X[col].unique() for col in X.columns}\n",
    "\n",
    "# Optional: print in readable format\n",
    "for col, values in distinct_values.items():\n",
    "    print(f\"{col} size: {len(values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3f80c",
   "metadata": {},
   "source": [
    "All except fnlwgt are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbe94533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishali Rathore\\AppData\\Local\\Temp\\ipykernel_4748\\650624975.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.drop('fnlwgt', inplace=True, axis=1)\n"
     ]
    }
   ],
   "source": [
    "X.drop('fnlwgt', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55a3b3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'education', 'education-num', 'marital-status',\n",
       "       'occupation', 'relationship', 'race', 'sex', 'capital-gain',\n",
       "       'capital-loss', 'hours-per-week', 'native-country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_columns = X.columns\n",
    "discrete_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52a82d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_transformer import DataTransformer\n",
    "\n",
    "transformer = DataTransformer()\n",
    "transformer.fit(X, discrete_columns)\n",
    "train_data = transformer.transform(X)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c0e5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SpanInfo(dim=74, activation_fn='softmax')],\n",
       " [SpanInfo(dim=10, activation_fn='softmax')],\n",
       " [SpanInfo(dim=16, activation_fn='softmax')],\n",
       " [SpanInfo(dim=16, activation_fn='softmax')],\n",
       " [SpanInfo(dim=7, activation_fn='softmax')],\n",
       " [SpanInfo(dim=16, activation_fn='softmax')],\n",
       " [SpanInfo(dim=6, activation_fn='softmax')],\n",
       " [SpanInfo(dim=5, activation_fn='softmax')],\n",
       " [SpanInfo(dim=2, activation_fn='softmax')],\n",
       " [SpanInfo(dim=123, activation_fn='softmax')],\n",
       " [SpanInfo(dim=99, activation_fn='softmax')],\n",
       " [SpanInfo(dim=96, activation_fn='softmax')],\n",
       " [SpanInfo(dim=43, activation_fn='softmax')]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.output_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09846740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_sampler import DataSampler\n",
    "\n",
    "data_sampler = DataSampler(train_data, transformer.output_info_list, log_frequency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2b3aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 513) (3, 13) [1 9 5] [ 4 31  1]\n"
     ]
    }
   ],
   "source": [
    "c1, m1, col, opt = data_sampler.sample_condvec(3)\n",
    "print(c1.shape, m1.shape, col, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_dim = 128\n",
    "generator_dim = 128\n",
    "discriminator_dim = 128\n",
    "data_dim = X.shape[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = Generator(embedding_dim + data_sampler.dim_cond_vec(), generator_dim, data_dim).to(device)\n",
    "discriminator = Discriminator(data_dim + data_sampler.dim_cond_vec(), discriminator_dim).to(device)\n",
    "\n",
    "optimizerG = torch.optim.Adam(\n",
    "            generator.parameters(),\n",
    "            betas=(0.5, 0.9),\n",
    "        )\n",
    "\n",
    "optimizerD = torch.optim.Adam(\n",
    "            discriminator.parameters(),\n",
    "            betas=(0.5, 0.9),\n",
    "        )\n",
    "\n",
    "mean = torch.zeros(batch_size, embedding_dim, device=device)\n",
    "std = mean + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a52ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(data_sampler)):\n",
    "        # Sample data\n",
    "        c1, m1, col, opt = data_sampler.sample_condvec(batch_size)\n",
    "        c1 = torch.from_numpy(c1).to(device)\n",
    "        m1 = torch.from_numpy(m1).to(device)\n",
    "        \n",
    "        fakez = torch.normal(mean=mean, std=std) # Sample noise\n",
    "        fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "        # Generate fake data\n",
    "        fake_data = generator(fakez)\n",
    "        fakeact = apply_activate(fake_data)\n",
    "\n",
    "        # Get real data\n",
    "        perm = np.arange(batch_size)\n",
    "        np.random.shuffle(perm)\n",
    "        real = data_sampler.sample_data(train_data, batch_size, col[perm], opt[perm])\n",
    "        c2 = c1[perm]\n",
    "\n",
    "        # Train Discriminator\n",
    "        fake_cat = torch.cat([fakeact, c1], dim=1)\n",
    "        real_cat = torch.cat([real, c2], dim=1)\n",
    "        y_fake = discriminator(fake_cat)\n",
    "        y_real = discriminator(real_cat)\n",
    "        \n",
    "        d_loss = -torch.mean(torch.log(y_real + 1e-8) + torch.log(1 - y_fake + 1e-8))\n",
    "        optimizerD.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizerG.zero_grad()\n",
    "        g_loss = -discriminator(fake_data, c1, m1).mean()\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ydata-profiling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
